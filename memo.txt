1. Overall System Architecture and Structure

The system is organized into three distinct layers to ensure efficient operation and resource management on the Raspberry Pi 4. The core optimization principle is **resource gating** (activating high-power components only when necessary).

Code Structure Breakdown:
- Sensor Layer: `ultra.py` and `pir.py` handle physical input via RPi.GPIO (Ultrasonic and PIR sensors).
- Core Logic Layer: `enroll.py` (Enrollment) and `recognize.py` (Authentication) contain the CPU-intensive computer vision algorithms.
- Integration Layer: `sensorRecognize.py` acts as the master script, controlling the state of the Core Logic based on input from the Sensor Layer.

Models and Data:
- Models: YuNet (`face_detection_yunet_2023mar.onnx`) for face detection, and MobileFaceNet (`w600k_mbf.onnx`, part of `buffalo_sc`) for lightweight feature embedding.
- User Data: Stored in `data/users/{USERNAME}/embeddings.npy` as a stacked matrix of feature vectors (not a single mean vector).

2. Detailed Analysis of `enroll.py` (Enrollment Script)

The structure of `enroll.py` uses a phased approach to capture a "manifold" of the user's face rather than a single snapshot.

A. Configuration and Quality Control Utilities:
- `TOTAL_SAMPLES = 40`: Specifies the required number of samples.
- `BLUR_THRESHOLD = 30.0`: A strict quality gate. The function `is_blurry(img)` rejects frames with low Laplacian variance.
- `STARE_DURATION = 4.0`: Duration for the initial static capture phase.
- `def l2_normalize(v)`: Standard L2-normalization for accurate cosine similarity.

B. Enrollment Phases (State Machine):
- **Phase 1: Countdown**: Gives the user 3 seconds to prepare.
- **Phase 2: Stare**: Captures ~15 samples while the user looks straight at the camera. This establishes a strong baseline for frontal recognition.
- **Phase 3: Rotate**: The user rotates their head. A **Diversity Check** is performed: if `sims.max() > 0.80` (current face is too similar to existing samples), the frame is rejected. This forces the collection of diverse angles.

C. Final Template Creation:
- Unlike previous versions, this script **does not calculate a mean**.
- It saves the raw stack of embeddings `np.stack(embeddings)` to `embeddings.npy`. This allows the recognition script to match against multiple specific angles rather than an averaged (and potentially blurry) representation.

3. Detailed Analysis of `recognize.py` (Authentication Script)

The structure of `recognize.py` emphasizes speed and stability, utilizing ONNX Runtime and matrix operations.

A. Configuration and Utilities:
- `THRESH = 0.65`: Stricter similarity threshold.
- Temporal Voting:
    - `VOTE_WINDOW = 10`: Larger window for stability.
    - `VOTE_PASS = 6`: Requires 6 positive votes out of 10 to authenticate.
- `SKIP_FRAMES = 2`: Processes only 1 out of every 3 frames to maintain UI responsiveness.

B. Inference and Comparison:
- **ONNX Runtime (ORT)**: The script attempts to use `onnxruntime` first for faster inference on the ARM CPU, falling back to `cv2.dnn` if unavailable.
- **Matrix Multiplication**: Instead of a loop, it uses `sims = templates @ vec` to compare the live face against all 40 enrolled samples of a user simultaneously.
- The maximum similarity score (`sims.max()`) determines the match quality.

4. Detailed Analysis of `sensorRecognize.py` (Complete System Integration)

The `sensorRecognize.py` script operates as a power-saving **state machine**, utilizing both PIR and Ultrasonic sensors to gate the camera.

A. Sensor Logic (SensorManager):
- **Ultrasonic (TRIG=23, ECHO=24)**: Measures distance to the user.
- **PIR (PIN=25)**: Detects infrared motion.
- **Trigger Condition**: The system considers a person present ONLY if `distance <= 70.0` cm AND `motion` is detected.

B. State Machine Structure:
1. **IDLE State**:
   - The camera is OFF. The system loops, polling the sensors every `0.1s`.
   - If the trigger condition is met, it transitions to ACTIVE.

2. **ACTIVE State**:
   - The camera initializes, and the face recognition loop (identical to `recognize.py`) begins.
   - **Timeout Logic**: If the sensors report no person (`distance > 70cm` or no motion), an 8-second timer (`IDLE_TIMEOUT = 8.0`) starts.
   - If the timer expires without the person returning, the camera is released (`cap.release()`), and the system returns to IDLE.

C. Optimization Summary:
This structure ensures the heavy ONNX models and video capture threads are only active when a user is physically within interaction range, significantly reducing thermal load and power consumption on the Pi 4.
